{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Keras imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing as preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_path = os.listdir(\"../Data/Images\")\n",
    "not_path.remove(\".DS_Store\") #it likes to pop up from time to time\n",
    "\n",
    "data_path = os.path.join('..', 'Data')\n",
    "\n",
    "train_dir = os.path.join(data_path, 'ExModeling_train')\n",
    "test_dir = os.path.join(data_path, 'ExModeling_test')\n",
    "val_dir = os.path.join(data_path, 'ExModeling_val')\n",
    "\n",
    "Ex_test_path = os.path.join(data_path, 'ExModeling_test')\n",
    "Ex_train_path = os.path.join(data_path, 'ExModeling_train')\n",
    "Ex_val_path = os.path.join(data_path, 'ExModeling_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vis method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_history(history): \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax[0].set_title('loss')\n",
    "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    ax[1].set_title('acc')\n",
    "    ax[1].plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n",
    "    ax[1].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (452, 411) #avg training image size, img gen is fine but model does not like the tuple for some reason keep pecking at it\n",
    "bch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 752 images belonging to 4 classes.\n",
      "Found 98 images belonging to 4 classes.\n",
      "Found 92 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "img_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=False, vertical_flip=False)\n",
    "\n",
    "train_generator = img_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size),\n",
    "    color_mode='rgb',\n",
    "    batch_size=bch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = img_gen.flow_from_directory(\n",
    "    val_dir, \n",
    "    target_size=(img_size), \n",
    "    color_mode='rgb',\n",
    "    batch_size=bch_size, \n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = img_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_size),\n",
    "    color_mode='rgb',\n",
    "    batch_size=bch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main changes here are going to be a better fitting image size and more epochs than the prior iterations of this model. A better fitting image size will in theory make the model better at assessing the images and I want to use more epochs as it did not seem quite done learning after 10 last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1022 20:33:06.311758 4725431744 deprecation_wrapper.py:119] From /Users/adamroth/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1022 20:33:06.345934 4725431744 deprecation_wrapper.py:119] From /Users/adamroth/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1022 20:33:06.353985 4725431744 deprecation_wrapper.py:119] From /Users/adamroth/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1022 20:33:06.410812 4725431744 deprecation_wrapper.py:119] From /Users/adamroth/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1022 20:33:06.564699 4725431744 deprecation_wrapper.py:119] From /Users/adamroth/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1022 20:33:06.576951 4725431744 deprecation_wrapper.py:119] From /Users/adamroth/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1022 20:33:06.792805 4725431744 deprecation.py:323] From /Users/adamroth/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1022 20:33:06.992810 4725431744 deprecation_wrapper.py:119] From /Users/adamroth/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "100/100 [==============================] - 8330s 83s/step - loss: 47.2761 - acc: 0.4052 - val_loss: 35.2329 - val_acc: 0.3878\n",
      "Epoch 2/15\n",
      "100/100 [==============================] - 2989s 30s/step - loss: 25.9263 - acc: 0.5782 - val_loss: 17.7986 - val_acc: 0.6327\n",
      "Epoch 3/15\n",
      "100/100 [==============================] - 1849s 18s/step - loss: 11.9269 - acc: 0.6545 - val_loss: 7.1291 - val_acc: 0.6429\n",
      "Epoch 4/15\n",
      "100/100 [==============================] - 1832s 18s/step - loss: 4.2523 - acc: 0.6932 - val_loss: 2.3080 - val_acc: 0.6531\n",
      "Epoch 5/15\n",
      "100/100 [==============================] - 1814s 18s/step - loss: 1.6863 - acc: 0.6940 - val_loss: 1.4160 - val_acc: 0.6531\n",
      "Epoch 6/15\n",
      "100/100 [==============================] - 1862s 19s/step - loss: 1.1794 - acc: 0.7492 - val_loss: 1.7015 - val_acc: 0.5306\n",
      "Epoch 7/15\n",
      "100/100 [==============================] - 1842s 18s/step - loss: 1.0284 - acc: 0.7725 - val_loss: 1.2681 - val_acc: 0.6327\n",
      "Epoch 8/15\n",
      "100/100 [==============================] - 1829s 18s/step - loss: 0.8942 - acc: 0.8102 - val_loss: 1.3374 - val_acc: 0.6429\n",
      "Epoch 9/15\n",
      "100/100 [==============================] - 1838s 18s/step - loss: 0.7568 - acc: 0.8622 - val_loss: 1.4585 - val_acc: 0.6531\n",
      "Epoch 10/15\n",
      "100/100 [==============================] - 1862s 19s/step - loss: 0.6502 - acc: 0.8910 - val_loss: 1.3886 - val_acc: 0.7143\n",
      "Epoch 11/15\n",
      "100/100 [==============================] - 1829s 18s/step - loss: 0.5593 - acc: 0.9230 - val_loss: 1.4999 - val_acc: 0.6735\n",
      "Epoch 12/15\n",
      "100/100 [==============================] - 1847s 18s/step - loss: 0.4696 - acc: 0.9493 - val_loss: 1.5957 - val_acc: 0.6633\n",
      "Epoch 13/15\n",
      "100/100 [==============================] - 1853s 19s/step - loss: 0.4475 - acc: 0.9630 - val_loss: 1.6578 - val_acc: 0.6224\n",
      "Epoch 14/15\n",
      "100/100 [==============================] - 1840s 18s/step - loss: 0.3691 - acc: 0.9765 - val_loss: 2.4103 - val_acc: 0.6020\n",
      "Epoch 15/15\n",
      "100/100 [==============================] - 1829s 18s/step - loss: 0.3653 - acc: 0.9765 - val_loss: 2.3110 - val_acc: 0.6224\n"
     ]
    }
   ],
   "source": [
    "alpha = models.Sequential() \n",
    "alpha.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(452, 411, 3)))\n",
    "alpha.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "alpha.add(layers.MaxPooling2D((2, 2)))\n",
    "alpha.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "alpha.add(layers.MaxPooling2D((2, 2)))\n",
    "alpha.add(layers.Conv2D(128, (3, 3)))\n",
    "alpha.add(layers.MaxPooling2D((2, 2)))\n",
    "alpha.add(layers.Conv2D(256, (3, 3), activation='relu', kernel_regularizer =tf.keras.regularizers.l1( l=0.01) ))\n",
    "alpha.add(layers.MaxPooling2D((2, 2)))\n",
    "alpha.add(layers.Flatten())\n",
    "alpha.add(layers.Dense(64, activation='relu'))\n",
    "alpha.add(layers.Dense(64, activation='relu'))\n",
    "alpha.add(layers.Dense(128, activation='relu',))\n",
    "alpha.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "alpha.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
    "\n",
    "history_alpha = alpha.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=15, \n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
